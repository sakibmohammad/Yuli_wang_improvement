{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CNN to predict subtrate stiffness"
      ],
      "metadata": {
        "id": "WSc6afpgfGj7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxwB-4ZHfgrI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe8f4863-4fec-4cdb-b822-6e9f49b968e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold: 1\n",
            "Accuracy:  100.00\n",
            "Precision: 1.00%\n",
            "Recall: 1.00%\n",
            "F1 Score: 1.00%\n",
            "Fold: 2\n",
            "Accuracy:  100.00\n",
            "Precision: 1.00%\n",
            "Recall: 1.00%\n",
            "F1 Score: 1.00%\n",
            "Fold: 3\n",
            "Accuracy:  90.00\n",
            "Precision: 1.00%\n",
            "Recall: 0.80%\n",
            "F1 Score: 0.89%\n",
            "Fold: 4\n",
            "Accuracy:  100.00\n",
            "Precision: 1.00%\n",
            "Recall: 1.00%\n",
            "F1 Score: 1.00%\n",
            "Fold: 5\n",
            "Accuracy:  100.00\n",
            "Precision: 1.00%\n",
            "Recall: 1.00%\n",
            "F1 Score: 1.00%\n",
            "\n",
            "Average accuracy across all folds: 98.00%\n",
            "Average precision across all folds: 1.00\n",
            "Average recall across all folds: 0.96\n",
            "Average F1 score across all folds: 0.98\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models import resnet50, ResNet50_Weights\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import preprocessing\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torchvision import transforms\n",
        "\n",
        "img_width, img_height = 128, 128\n",
        "num_classes = 1\n",
        "seed = 42\n",
        "cv_splits = 5\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "do_not_keep_frozen = True\n",
        "\n",
        "initial_lr = 1e-3\n",
        "LR_patience = 5\n",
        "\n",
        "class NumpyToTensorTransform:\n",
        "    def __call__(self, pic):\n",
        "        return torch.from_numpy(np.array(pic, np.float32, copy=False)).permute(2, 0, 1)\n",
        "\n",
        "\n",
        "# Define the training and test transforms\n",
        "train_transforms = transforms.Compose([\n",
        "    NumpyToTensorTransform(),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomVerticalFlip(p=0.5),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.RandomResizedCrop((img_width, img_height), scale=(0.8, 1.0), antialias= True),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    NumpyToTensorTransform(),\n",
        "])\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = torch.LongTensor(targets)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "\n",
        "        # ensure the numpy array has the shape HxWxC\n",
        "        if x.shape[0] == 1:\n",
        "            x = np.transpose(x, (1, 2, 0))\n",
        "\n",
        "        # Normalize to 0-1\n",
        "        #x = x / 65535.0\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "# Function to load images and labels\n",
        "def load_images(directory):\n",
        "    image_paths = glob.glob(os.path.join(directory, '*.tif'))\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        image = Image.open(image_path)\n",
        "        image = image.resize((img_width, img_height))\n",
        "        image = np.array(image)\n",
        "        image = np.stack((image,)*3, axis= -1) #original images are single channel, making it 3 channels\n",
        "        # Append the image to the list\n",
        "        images.append(image)\n",
        "\n",
        "        # Extract the label from the directory name\n",
        "        label = os.path.basename(os.path.dirname(image_path))\n",
        "        labels.append(label)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Load the images and labels\n",
        "data_dir = '/content/drive/MyDrive/Just the images/*'  # Replace with the path to your data directory\n",
        "x_data, y_data = load_images(data_dir)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=cv_splits, shuffle=True, random_state=seed)\n",
        "fold = 1\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "for train_index, test_index in skf.split(x_data, y_data):\n",
        "    print(f\"Fold: {fold}\")\n",
        "\n",
        "    # Split the data into train and validation sets for this fold\n",
        "    x_train, x_test = x_data[train_index], x_data[test_index]\n",
        "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
        "\n",
        "    #Encode labels from text to integers.\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(y_train)\n",
        "    y_train = le.transform(y_train)\n",
        "    y_test = le.transform(y_test)\n",
        "\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    train_data = Dataset(x_train, y_train, transform=train_transforms)\n",
        "    test_data = Dataset(x_test, y_test, transform = test_transforms)\n",
        "\n",
        "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
        "    for param in model.parameters():\n",
        "      param.requires_grad = keep_frozen\n",
        "\n",
        "    model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience= LR_patience)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data[0].to(device)/255.0, data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs.view(-1), labels.float())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step(loss)\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    preds = []\n",
        "    actual = []\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data[0].to(device)/255.0, data[1].to(device)\n",
        "            outputs = model(images)\n",
        "            predicted = (torch.sigmoid(outputs.view(-1)) > 0.5).float()\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Append the labels and predictions for precision, recall and F1 score\n",
        "            preds += predicted.tolist()\n",
        "            actual += labels.tolist()\n",
        "\n",
        "    fold += 1\n",
        "    accuracy_ = correct / total\n",
        "    print(f'Accuracy: {accuracy_ * 100: 0.2f}')\n",
        "    precision = precision_score(actual, preds)\n",
        "    recall = recall_score(actual, preds)\n",
        "    f1 = f1_score(actual, preds)\n",
        "    print(f'Precision: {precision:.2f}')\n",
        "    print(f'Recall: {recall:.2f}')\n",
        "    print(f'F1 Score: {f1:.2f}')\n",
        "\n",
        "    # Append the precision, recall, F1 score and accuracy for this fold to the lists\n",
        "    precision_list.append(precision)\n",
        "    recall_list.append(recall)\n",
        "    f1_list.append(f1)\n",
        "    accuracy_list.append(accuracy_)\n",
        "\n",
        "# Compute the average precision, recall, F1 score and accuracy across all folds\n",
        "avg_precision = sum(precision_list) / len(precision_list)\n",
        "avg_recall = sum(recall_list) / len(recall_list)\n",
        "avg_f1_score = sum(f1_list) / len(f1_list)\n",
        "avg_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "\n",
        "print(f'\\nAverage accuracy across all folds: {avg_accuracy*100:.2f}')\n",
        "print(f'Average precision across all folds: {avg_precision:.2f}')\n",
        "print(f'Average recall across all folds: {avg_recall:.2f}')\n",
        "print(f'Average F1 score across all folds: {avg_f1_score:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YfxP7Iq1fFQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNET to predict traction field from phase image"
      ],
      "metadata": {
        "id": "SctjWOnEENbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
        "import segmentation_models_pytorch as smp\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self, encoder_name='resnet34', encoder_weights='imagenet'):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.unet = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=1,\n",
        "            classes=2  # For 2D vector output\n",
        "        )\n",
        "        self.fc1 = nn.Linear(128 * 128 * 2, 256)  # Adjust dimensions accordingly\n",
        "        self.fc2 = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unet(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the output tensor\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def load_data(img_dir, csv_dir):\n",
        "    img_filenames = sorted(os.listdir(img_dir))\n",
        "    csv_filenames = sorted(os.listdir(csv_dir))\n",
        "    images = []\n",
        "    vectors = []\n",
        "    vector_scaler = MinMaxScaler()  # Initialize MinMaxScaler for vector data\n",
        "\n",
        "    for img_file, csv_file in zip(img_filenames, csv_filenames):\n",
        "        img_path = os.path.join(img_dir, img_file)\n",
        "        csv_path = os.path.join(csv_dir, csv_file)\n",
        "\n",
        "        image = Image.open(img_path).convert('L')\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(128),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "        ])\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "        csv_data = pd.read_csv(csv_path, usecols=[2, 3]).values\n",
        "        vectors.append(csv_data)\n",
        "\n",
        "    vector_data = np.vstack(vectors)  # Stack vectors to fit the scaler\n",
        "    vector_scaler.fit(vector_data)  # Fit the MinMaxScaler\n",
        "    vectors = [vector_scaler.transform(vec) for vec in vectors]  # Normalize vectors\n",
        "\n",
        "    images = torch.stack(images)  # Convert list of tensors to a single tensor\n",
        "    vectors = [torch.tensor(vec, dtype=torch.float32) for vec in vectors]  # Convert numpy arrays to tensors\n",
        "    vectors = torch.cat(vectors)  # Concatenate list of tensors to a single tensor\n",
        "\n",
        "    return images, vectors, vector_scaler\n",
        "\n",
        "# Load data\n",
        "img_dir = '/content/drive/MyDrive/Regression_images_traction/Images'\n",
        "csv_dir = '/content/drive/MyDrive/Regression_images_traction/Traction'\n",
        "images, vectors, vector_scaler = load_data(img_dir, csv_dir)\n",
        "images_np = images.numpy()\n",
        "vectors_np = vectors.numpy()\n",
        "\n",
        "# Prepare for 5-fold cross-validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(images)):\n",
        "\n",
        "    train_images = torch.tensor(images_np[train_idx], dtype=torch.float32)\n",
        "    val_images = torch.tensor(images_np[val_idx], dtype=torch.float32)\n",
        "    train_vectors = torch.tensor(vectors_np[train_idx], dtype=torch.float32)\n",
        "    val_vectors = torch.tensor(vectors_np[val_idx], dtype=torch.float32)\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(train_images, train_vectors), batch_size=4, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(val_images, val_vectors), batch_size=4)\n",
        "\n",
        "    # Model, loss function, optimizer\n",
        "    regressor_model = Regressor()\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    optimizer = optim.Adam(regressor_model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Step decay LR scheduler\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):\n",
        "        regressor_model.train()\n",
        "        for batch in train_loader:\n",
        "            images, traction_fields = batch\n",
        "            optimizer.zero_grad()\n",
        "            traction_preds = regressor_model(images)\n",
        "            loss = criterion(traction_preds, traction_fields)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        scheduler.step()  # Step decay\n",
        "\n",
        "    # Validation and metrics computation\n",
        "    regressor_model.eval()\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images, _ = batch\n",
        "            traction_preds = regressor_model(images)\n",
        "            val_preds.append(traction_preds)\n",
        "    val_preds = torch.cat(val_preds).numpy()\n",
        "    val_preds = vector_scaler.inverse_transform(val_preds)  # Inverse transform\n",
        "    val_vectors = vector_scaler.inverse_transform(val_vectors.numpy())  # Inverse transform\n",
        "\n",
        "    mae = mean_absolute_error(val_vectors, val_preds)\n",
        "    mape = mean_absolute_percentage_error(val_vectors, val_preds)\n",
        "    fold_results.append((mae, mape))\n",
        "\n",
        "# Output results\n",
        "for fold, (mae, mape) in enumerate(fold_results):\n",
        "    print(f'Fold {fold + 1}: MAE = {mae}, MAPE = {mape}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRXUXRBxEMme",
        "outputId": "a4c7c9da-e3f1-42ee-cc14-28eb0ba21ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.16.0+cu118)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.1)\n",
            "Requirement already satisfied: timm==0.9.2 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.18.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n",
            "Fold 1: MAE = 5.904709499127136e-10, MAPE = 106655.703125%\n",
            "Fold 2: MAE = 1.4391685065895388e-10, MAPE = 37179.69921875%\n",
            "Fold 3: MAE = 3.0239830084433095e-10, MAPE = 107908.96875%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNET to predict traction vectors from area and bead displacement vectors"
      ],
      "metadata": {
        "id": "LVLfmOb9fUVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
        "import segmentation_models_pytorch as smp\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    def __init__(self, encoder_name='resnet34', encoder_weights='imagenet'):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.unet = smp.Unet(\n",
        "            encoder_name=encoder_name,\n",
        "            encoder_weights=encoder_weights,\n",
        "            in_channels=2,\n",
        "            classes=2\n",
        "        )\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1935, 2))  # Adjust the pooling layer to match target dimensions\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.unet(x)\n",
        "        x = self.adaptive_pool(x)  # Apply Adaptive Average Pooling to match target dimensions\n",
        "        x = x[..., 0]  # Remove the last dimension by selecting the first index along that dimension\n",
        "        return x\n",
        "\n",
        "def load_data(csv_dir):\n",
        "    csv_filenames = sorted(os.listdir(csv_dir))\n",
        "    inputs1 = []  # For columns 0, 1\n",
        "    inputs2 = []  # For columns 4, 5\n",
        "    targets = []  # For columns 2, 3\n",
        "\n",
        "    input_scaler1 = MinMaxScaler()  # Initialize MinMaxScaler for input data (columns 0, 1)\n",
        "    input_scaler2 = MinMaxScaler()  # Initialize MinMaxScaler for input data (columns 4, 5)\n",
        "    target_scaler = MinMaxScaler()  # Initialize MinMaxScaler for target data (columns 2, 3)\n",
        "\n",
        "    max_rows = 0\n",
        "    for csv_file in csv_filenames:\n",
        "        csv_path = os.path.join(csv_dir, csv_file)\n",
        "        csv_data = pd.read_csv(csv_path, usecols=[0, 1, 2, 3, 4, 5])\n",
        "        max_rows = max(max_rows, len(csv_data))\n",
        "\n",
        "    for csv_file in csv_filenames:\n",
        "        csv_path = os.path.join(csv_dir, csv_file)\n",
        "        csv_data = pd.read_csv(csv_path, usecols=[0, 1, 2, 3, 4, 5])\n",
        "        csv_data.fillna(0, inplace=True)  # Fill NaN values with 0\n",
        "        csv_data = csv_data.values\n",
        "\n",
        "        # Pad the data with zeros to match the largest array\n",
        "        padded_data = np.zeros((max_rows, csv_data.shape[1]))\n",
        "        padded_data[:csv_data.shape[0], :] = csv_data\n",
        "\n",
        "        inputs1.append(padded_data[:, :2])  # Get columns 0, 1\n",
        "        inputs2.append(padded_data[:, 4:])  # Get columns 4, 5\n",
        "        targets.append(padded_data[:, 2:4])  # Get columns 2, 3 as target\n",
        "\n",
        "    # Normalize data and convert to tensors\n",
        "    inputs1 = [input_scaler1.fit_transform(i) for i in inputs1]\n",
        "    inputs2 = [input_scaler2.fit_transform(i) for i in inputs2]\n",
        "    targets = [target_scaler.fit_transform(t) for t in targets]\n",
        "\n",
        "    return inputs1, inputs2, targets, input_scaler1, input_scaler2, target_scaler\n",
        "\n",
        "\n",
        "\n",
        "csv_dir = '/content/drive/MyDrive/Clean_only_CSV/Regression'\n",
        "inputs1, inputs2, targets, input_scaler1, input_scaler2, target_scaler = load_data(csv_dir)\n",
        "\n",
        "max_len = max(max(len(i) for i in inputs1), max(len(i) for i in inputs2))\n",
        "# Pad to ensure dimensions are divisible by 32\n",
        "max_len_padded = ((max_len + 31) // 32) * 32\n",
        "inputs1 = [np.pad(i, ((0, max_len_padded - len(i)), (0, 30))) for i in inputs1]  # Pad width to 32\n",
        "inputs2 = [np.pad(i, ((0, max_len_padded - len(i)), (0, 30))) for i in inputs2]  # Pad width to 32\n",
        "\n",
        "\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "inputs1_np = np.array(inputs1, dtype=np.float32)\n",
        "inputs2_np = np.array(inputs2, dtype=np.float32)\n",
        "inputs1_np = inputs1_np.reshape(-1, 1, max_len_padded, 32)\n",
        "inputs2_np = inputs2_np.reshape(-1, 1, max_len_padded, 32)\n",
        "targets_np = np.array(targets, dtype=np.float32)\n",
        "\n",
        "# Reshape the input data to have a single channel (to match the U-Net input requirements)\n",
        "\n",
        "# Concatenate along the channel dimension\n",
        "inputs_np = np.concatenate((inputs1_np, inputs2_np), axis=1)\n",
        "\n",
        "# Check the shapes of your arrays\n",
        "print(f'inputs_np shape: {inputs_np.shape}')\n",
        "print(f'targets_np shape: {targets_np.shape}')\n",
        "\n",
        "# Ensure the number of samples matches between inputs and targets\n",
        "assert inputs_np.shape[0] == targets_np.shape[0], \"Mismatched number of samples between inputs and targets\"\n",
        "\n",
        "# Prepare for 5-fold cross-validation\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "def pad_to_match_shapes(tensor1, tensor2):\n",
        "    # Get the maximum dimensions\n",
        "    max_height = max(tensor1.size(1), tensor2.size(1))\n",
        "    max_width = max(tensor1.size(2), tensor2.size(2))\n",
        "\n",
        "    # Pad tensors\n",
        "    pad_tensor1 = nn.functional.pad(tensor1, (0, max_width - tensor1.size(2), 0, max_height - tensor1.size(1)))\n",
        "    pad_tensor2 = nn.functional.pad(tensor2, (0, max_width - tensor2.size(2), 0, max_height - tensor2.size(1)))\n",
        "\n",
        "    return pad_tensor1, pad_tensor2\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(inputs_np)):\n",
        "    # Get training and validation data\n",
        "    train_inputs = torch.tensor(inputs_np[train_idx], dtype=torch.float32)\n",
        "    val_inputs = torch.tensor(inputs_np[val_idx], dtype=torch.float32)\n",
        "    train_targets = torch.tensor(targets_np[train_idx], dtype=torch.float32)\n",
        "    val_targets = torch.tensor(targets_np[val_idx], dtype=torch.float32)\n",
        "\n",
        "    # Create DataLoader instances\n",
        "    train_loader = DataLoader(TensorDataset(train_inputs, train_targets), batch_size=4, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(val_inputs, val_targets), batch_size=4)\n",
        "\n",
        "    # Model, loss function, optimizer\n",
        "    regressor_model = Regressor()\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    optimizer = optim.Adam(regressor_model.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Step decay LR scheduler\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(10):\n",
        "      regressor_model.train()\n",
        "      for batch in train_loader:\n",
        "        images, traction_fields = batch\n",
        "        optimizer.zero_grad()\n",
        "        traction_preds = regressor_model(images)\n",
        "\n",
        "        # Remove the extra dimension from traction_preds\n",
        "        traction_preds = traction_preds.squeeze(-1)\n",
        "\n",
        "        # Now traction_preds should have a shape of [4, 1935, 2]\n",
        "\n",
        "        # Pad outputs and targets to match shapes\n",
        "        traction_preds, traction_fields = pad_to_match_shapes(traction_preds, traction_fields)\n",
        "        #print(f'traction_preds dimensions: {traction_preds.size()}')\n",
        "        #print(f'traction_fields dimensions: {traction_fields.size()}')\n",
        "        # Calculate loss\n",
        "        loss = criterion(traction_preds, traction_fields)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      scheduler.step()  # Step decay\n",
        "\n",
        "    # Validation and metrics computation\n",
        "    regressor_model.eval()\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images, _ = batch\n",
        "            traction_preds = regressor_model(images)\n",
        "            val_preds.append(traction_preds)\n",
        "    val_preds = torch.cat(val_preds).numpy()\n",
        "\n",
        "    # Inverse transform the predictions and targets for metric calculations\n",
        "    val_preds = target_scaler.inverse_transform(val_preds.reshape(-1, 2))\n",
        "    val_targets = target_scaler.inverse_transform(val_targets.numpy().reshape(-1, 2))\n",
        "\n",
        "    mae = mean_absolute_error(val_targets, val_preds)\n",
        "    mape = mean_absolute_percentage_error(val_targets, val_preds)\n",
        "    fold_results.append((mae, mape))\n",
        "\n",
        "# Output results\n",
        "for fold, (mae, mape) in enumerate(fold_results):\n",
        "    print(f'Fold {fold + 1}: MAE = {mae}, MAPE = {mape}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe6vNEBja2WD",
        "outputId": "02968d0d-371f-43b2-bbb9-da35299a078b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.16.0+cu118)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.4)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.7.1 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.7.1)\n",
            "Requirement already satisfied: timm==0.9.2 in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (0.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (4.66.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from segmentation_models_pytorch) (9.4.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0+cu118)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.10/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.0.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.18.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm==0.9.2->segmentation_models_pytorch) (0.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (2.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm==0.9.2->segmentation_models_pytorch) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->segmentation_models_pytorch) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch==0.7.1->segmentation_models_pytorch) (1.3.0)\n",
            "inputs_np shape: (52, 2, 1952, 32)\n",
            "targets_np shape: (52, 1935, 2)\n",
            "Fold 1: MAE = 9.832324190939268e-12, MAPE = 17.874073028564453%\n",
            "Fold 2: MAE = 1.015167290507879e-11, MAPE = 16.227169036865234%\n",
            "Fold 3: MAE = 1.0226283361780553e-11, MAPE = 1704.95703125%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "UNET to predict traction vectors from both phase image and bead displacement and area"
      ],
      "metadata": {
        "id": "bGt6Kwx7fj_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install segmentation_models_pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Normalize\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
        "import segmentation_models_pytorch as smp\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class MultiModalRegressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiModalRegressor, self).__init__()\n",
        "        # Image U-Net\n",
        "        self.image_unet = smp.Unet(encoder_name='resnet34', encoder_weights='imagenet', in_channels=1, classes=2)\n",
        "        # CSV U-Net\n",
        "        self.csv_unet = smp.Unet(encoder_name='resnet34', encoder_weights='imagenet', in_channels=2, classes=2)\n",
        "        # Fusion and Prediction\n",
        "        self.image_fc = nn.Linear(2, 4)\n",
        "        self.fc_fusion = nn.Linear(6, 256)  # Adjust dimensions accordingly\n",
        "        self.fc_prediction = nn.Linear(256, 2)\n",
        "\n",
        "\n",
        "    def forward(self, image_input, csv_input):\n",
        "      image_features = self.image_unet(image_input)\n",
        "      image_features = torch.mean(image_features, dim=[2, 3])\n",
        "      image_features = self.image_fc(image_features)\n",
        "      csv_features = self.csv_unet(csv_input)\n",
        "      csv_features = torch.mean(csv_features, dim=[2, 3])\n",
        "\n",
        "\n",
        "\n",
        "      # Concatenate features\n",
        "      fused_features = torch.cat((image_features, csv_features), dim=1)\n",
        "\n",
        "      # Fusion layer\n",
        "      fused_features = nn.ReLU()(self.fc_fusion(fused_features))\n",
        "\n",
        "      # Prediction\n",
        "      predictions = self.fc_prediction(fused_features)\n",
        "\n",
        "      return predictions\n",
        "\n",
        "\n",
        "def load_data_img(img_dir, csv_dir):\n",
        "    img_filenames = sorted(os.listdir(img_dir))\n",
        "    csv_filenames = sorted(os.listdir(csv_dir))\n",
        "    images = []\n",
        "    vectors = []\n",
        "    vector_scaler = MinMaxScaler()  # Initialize MinMaxScaler for vector data\n",
        "\n",
        "    for img_file, csv_file in zip(img_filenames, csv_filenames):\n",
        "        img_path = os.path.join(img_dir, img_file)\n",
        "        csv_path = os.path.join(csv_dir, csv_file)\n",
        "\n",
        "        image = Image.open(img_path).convert('L')\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(128),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
        "        ])\n",
        "        image = transform(image)\n",
        "        images.append(image)\n",
        "\n",
        "        csv_data = pd.read_csv(csv_path, usecols=[2, 3]).values\n",
        "        vectors.append(csv_data)\n",
        "\n",
        "    vector_data = np.vstack(vectors)  # Stack vectors to fit the scaler\n",
        "    vector_scaler.fit(vector_data)  # Fit the MinMaxScaler\n",
        "    vectors = [vector_scaler.transform(vec) for vec in vectors]  # Normalize vectors\n",
        "\n",
        "    images = torch.stack(images)  # Convert list of tensors to a single tensor\n",
        "    vectors = [torch.tensor(vec, dtype=torch.float32) for vec in vectors]  # Convert numpy arrays to tensors\n",
        "    vectors = torch.cat(vectors)  # Concatenate list of tensors to a single tensor\n",
        "\n",
        "    return images, vectors, vector_scaler\n",
        "\n",
        "\n",
        "def load_data_csv(csv_dir):\n",
        "    csv_filenames = sorted(os.listdir(csv_dir))\n",
        "    inputs1 = []  # For columns 0, 1\n",
        "    inputs2 = []  # For columns 4, 5\n",
        "    targets = []  # For columns 2, 3\n",
        "\n",
        "    input_scaler1 = MinMaxScaler()  # Initialize MinMaxScaler for input data (columns 0, 1)\n",
        "    input_scaler2 = MinMaxScaler()  # Initialize MinMaxScaler for input data (columns 4, 5)\n",
        "    target_scaler1 = MinMaxScaler()  # For column 2\n",
        "    target_scaler2 = MinMaxScaler()  # Initialize MinMaxScaler for target data (columns 2, 3)\n",
        "\n",
        "    max_rows = 0\n",
        "    for csv_file in csv_filenames:\n",
        "        csv_path = os.path.join(csv_dir, csv_file)\n",
        "        csv_data = pd.read_csv(csv_path, usecols=[0, 1, 2, 3, 4, 5])\n",
        "        max_rows = max(max_rows, len(csv_data))\n",
        "\n",
        "    for csv_file in csv_filenames:\n",
        "        csv_path = os.path.join(csv_dir, csv_file)\n",
        "        csv_data = pd.read_csv(csv_path, usecols=[0, 1, 2, 3, 4, 5])\n",
        "        csv_data.fillna(0, inplace=True)  # Fill NaN values with 0\n",
        "        csv_data = csv_data.values\n",
        "\n",
        "        # Pad the data with zeros to match the largest array\n",
        "        padded_data = np.zeros((max_rows, csv_data.shape[1]))\n",
        "        padded_data[:csv_data.shape[0], :] = csv_data\n",
        "\n",
        "        inputs1.append(padded_data[:, :2])  # Get columns 0, 1\n",
        "        inputs2.append(padded_data[:, 4:])  # Get columns 4, 5\n",
        "        targets.append(padded_data[:, 2:4])  # Get columns 2, 3 as target\n",
        "\n",
        "    # Normalize data and convert to tensors\n",
        "    inputs1 = [input_scaler1.fit_transform(i) for i in inputs1]\n",
        "    inputs2 = [input_scaler2.fit_transform(i) for i in inputs2]\n",
        "    targets1 = [target_scaler1.fit_transform(t[:, 0].reshape(-1, 1)) for t in targets]\n",
        "    targets2 = [target_scaler2.fit_transform(t[:, 1].reshape(-1, 1)) for t in targets]\n",
        "    targets = [np.hstack((t1, t2)) for t1, t2 in zip(targets1, targets2)]\n",
        "\n",
        "    return inputs1, inputs2, targets, input_scaler1, input_scaler2, target_scaler1, target_scaler2\n",
        "\n",
        "\n",
        "csv_dir = '/content/drive/MyDrive/Clean_only_CSV/Regression'\n",
        "inputs1, inputs2, targets, input_scaler1, input_scaler2, target_scaler1, target_scaler2 = load_data_csv(csv_dir)\n",
        "\n",
        "max_len = max(max(len(i) for i in inputs1), max(len(i) for i in inputs2))\n",
        "# Pad to ensure dimensions are divisible by 32\n",
        "max_len_padded = ((max_len + 31) // 32) * 32\n",
        "inputs1 = [np.pad(i, ((0, max_len_padded - len(i)), (0, 30))) for i in inputs1]  # Pad width to 32\n",
        "inputs2 = [np.pad(i, ((0, max_len_padded - len(i)), (0, 30))) for i in inputs2]  # Pad width to 32\n",
        "\n",
        "\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "inputs1_np = np.array(inputs1, dtype=np.float32)\n",
        "inputs2_np = np.array(inputs2, dtype=np.float32)\n",
        "inputs1_np = inputs1_np.reshape(-1, 1, max_len_padded, 32)\n",
        "inputs2_np = inputs2_np.reshape(-1, 1, max_len_padded, 32)\n",
        "targets_np = np.array(targets, dtype=np.float32)\n",
        "\n",
        "# Reshape the input data to have a single channel (to match the U-Net input requirements)\n",
        "\n",
        "# Concatenate along the channel dimension\n",
        "inputs_np = np.concatenate((inputs1_np, inputs2_np), axis=1)\n",
        "\n",
        "# Check the shapes of your arrays\n",
        "print(f'inputs_np shape: {inputs_np.shape}')\n",
        "print(f'targets_np shape: {targets_np.shape}')\n",
        "\n",
        "# Ensure the number of samples matches between inputs and targets\n",
        "assert inputs_np.shape[0] == targets_np.shape[0], \"Mismatched number of samples between inputs and targets\"\n",
        "\n",
        "\n",
        "# Load data\n",
        "img_dir = '/content/drive/MyDrive/Regression_images_traction/Images'\n",
        "csv_dir = '/content/drive/MyDrive/Regression_images_traction/Traction'\n",
        "images, vectors, vector_scaler = load_data_img(img_dir, csv_dir)\n",
        "images_np = images.numpy()\n",
        "vectors_np = vectors.numpy()\n",
        "\n",
        "\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(inputs_np)):\n",
        "    # Get training and validation data\n",
        "    train_inputs = torch.tensor(inputs_np[train_idx], dtype=torch.float32)\n",
        "    val_inputs = torch.tensor(inputs_np[val_idx], dtype=torch.float32)\n",
        "    train_targets = torch.tensor(targets_np[train_idx], dtype=torch.float32)\n",
        "    val_targets = torch.tensor(targets_np[val_idx], dtype=torch.float32)\n",
        "\n",
        "    # Create DataLoader instances\n",
        "    train_loader = DataLoader(TensorDataset(images[train_idx], train_inputs, train_targets), batch_size=4, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(images[val_idx], val_inputs, val_targets), batch_size=4)\n",
        "    # Model, loss function, optimizer\n",
        "    multi_modal_regressor = MultiModalRegressor()\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    optimizer = optim.Adam(multi_modal_regressor.parameters(), lr=0.001)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)  # Step decay LR scheduler\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(1):\n",
        "      multi_modal_regressor.train()\n",
        "      for img_data, csv_data, targets in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          predictions = multi_modal_regressor(img_data, csv_data)\n",
        "\n",
        "          # Assuming your targets are in the csv_data (if not, adjust accordingly)\n",
        "          loss = criterion(predictions, targets.mean(dim=1))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "# Evaluation\n",
        "    multi_modal_regressor.eval()\n",
        "    val_preds_list = []\n",
        "    val_targets_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for img_data, csv_data, targets in val_loader:\n",
        "        targets = targets[:, 0, :]\n",
        "        predictions = multi_modal_regressor(img_data, csv_data)\n",
        "        print(f\"Predictions shape for this batch: {predictions.shape}\")\n",
        "        print(f\"Targets shape for this batch: {targets.shape}\")\n",
        "        val_preds_list.append(predictions)\n",
        "        val_targets_list.append(targets)\n",
        "        print(f\"Total number of predictions so far: {len(val_preds_list) * predictions.shape[0]}\")\n",
        "        print(f\"Total number of targets so far: {len(val_targets_list) * targets.shape[0]}\")\n",
        "\n",
        "\n",
        "    val_preds = torch.cat(val_preds_list, dim=0)\n",
        "    print(f\"val_preds shape: {val_preds.shape}\")  # Add this line\n",
        "    val_targets = torch.cat(val_targets_list, dim=0)\n",
        "\n",
        "\n",
        "    # Stack predictions and targets\n",
        "    val_preds_aggregated1 = val_preds[:, 0].numpy()\n",
        "    val_preds_aggregated2 = val_preds[:, 1].numpy()\n",
        "    val_targets_aggregated1 = val_targets[:, 0].numpy()\n",
        "    val_targets_aggregated2 = val_targets[:, 1].numpy()\n",
        "\n",
        "    val_preds_aggregated1 = target_scaler1.inverse_transform(val_preds_aggregated1.reshape(-1, 1)).flatten()\n",
        "    val_preds_aggregated2 = target_scaler2.inverse_transform(val_preds_aggregated2.reshape(-1, 1)).flatten()\n",
        "    val_targets_aggregated1 = target_scaler1.inverse_transform(val_targets_aggregated1.reshape(-1, 1)).flatten()\n",
        "    val_targets_aggregated2 = target_scaler2.inverse_transform(val_targets_aggregated2.reshape(-1, 1)).flatten()\n",
        "\n",
        "    val_preds_aggregated = np.column_stack((val_preds_aggregated1, val_preds_aggregated2))\n",
        "    val_targets_aggregated = np.column_stack((val_targets_aggregated1, val_targets_aggregated2))\n",
        "\n",
        "    assert val_preds_aggregated.shape == val_targets_aggregated.shape, f\"Mismatched shapes: {val_preds_aggregated.shape} vs {val_targets_aggregated.shape}\"\n",
        "\n",
        "    # Now compute the error\n",
        "    mae = mean_absolute_error(val_targets_aggregated, val_preds_aggregated)\n",
        "    mape = mean_absolute_percentage_error(val_targets_aggregated, val_preds_aggregated)\n",
        "    fold_results.append((mae, mape))\n",
        "# Output results\n",
        "for fold, (mae, mape) in enumerate(fold_results):\n",
        "    print(f'Fold {fold + 1}: MAE = {mae}, MAPE = {mape}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB7eTTFa7BNH",
        "outputId": "88bf4fb2-1088-41c2-c1b6-14389e45188f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs_np shape: (52, 2, 1952, 32)\n",
            "targets_np shape: (52, 1935, 2)\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 4\n",
            "Total number of targets so far: 4\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 8\n",
            "Total number of targets so far: 8\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 12\n",
            "Total number of targets so far: 12\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 16\n",
            "Total number of targets so far: 16\n",
            "Predictions shape for this batch: torch.Size([2, 2])\n",
            "Targets shape for this batch: torch.Size([2, 2])\n",
            "Total number of predictions so far: 10\n",
            "Total number of targets so far: 10\n",
            "val_preds shape: torch.Size([18, 2])\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 4\n",
            "Total number of targets so far: 4\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 8\n",
            "Total number of targets so far: 8\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 12\n",
            "Total number of targets so far: 12\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 16\n",
            "Total number of targets so far: 16\n",
            "Predictions shape for this batch: torch.Size([1, 2])\n",
            "Targets shape for this batch: torch.Size([1, 2])\n",
            "Total number of predictions so far: 5\n",
            "Total number of targets so far: 5\n",
            "val_preds shape: torch.Size([17, 2])\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 4\n",
            "Total number of targets so far: 4\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 8\n",
            "Total number of targets so far: 8\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 12\n",
            "Total number of targets so far: 12\n",
            "Predictions shape for this batch: torch.Size([4, 2])\n",
            "Targets shape for this batch: torch.Size([4, 2])\n",
            "Total number of predictions so far: 16\n",
            "Total number of targets so far: 16\n",
            "Predictions shape for this batch: torch.Size([1, 2])\n",
            "Targets shape for this batch: torch.Size([1, 2])\n",
            "Total number of predictions so far: 5\n",
            "Total number of targets so far: 5\n",
            "val_preds shape: torch.Size([17, 2])\n",
            "Fold 1: MAE = 1.1512057973561696e-09, MAPE = 2046.458251953125%\n",
            "Fold 2: MAE = 2.005003107985459e-10, MAPE = 290.73602294921875%\n",
            "Fold 3: MAE = 4.2655606824482106e-10, MAPE = 1035.8106689453125%\n"
          ]
        }
      ]
    }
  ]
}